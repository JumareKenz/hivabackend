version: '3.8'
services:
  hiva-llm-engine:
    build: .
    network_mode: "host"
    environment:
      # Set to remote GPU server URL (e.g., "http://192.168.1.100:11434")
      # or keep as localhost if running Ollama locally
      - LLM_API_URL=${LLM_API_URL:-http://localhost:11434}
